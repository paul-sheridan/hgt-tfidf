{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcQTafDcJbBx"
   },
   "source": [
    "# Twenty Newsgroups Analysis\n",
    "\n",
    "Author: Paul Sheridan\n",
    "\n",
    "Goal: Run multinomial Naive Bayes classifier on Twenty Newsgroups dataset to using TF, TF-IDF, and hypergeometric test derived features, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1684929723878,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "w8P7Rg6ukLmh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp311-cp311-macosx_12_0_arm64.whl (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17.3\n",
      "  Downloading numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp311-cp311-macosx_12_0_arm64.whl (28.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.7/28.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "Successfully installed joblib-1.2.0 numpy-1.24.3 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCrl4H6Y85bp"
   },
   "source": [
    "## Custom TF-IDF Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MiDTNpYp891x"
   },
   "outputs": [],
   "source": [
    "class CanonicalTfidfTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "      # Initialize scoring matrix\n",
    "      d, m = X.shape\n",
    "      Nj = X.sum(axis=1)\n",
    "      tfidf = np.empty(shape=(d, m), dtype=np.float32)\n",
    "      \n",
    "      # Calculate IDF scores\n",
    "      a = X.nonzero()[1]\n",
    "      indices, counts = np.unique(a, return_counts=True)\n",
    "      Bi = np.zeros(m)\n",
    "      for i in range(len(indices)):\n",
    "        index = indices[i]\n",
    "        Bi[index] = counts[i]\n",
    "      for i in range(m):\n",
    "        if Bi[i] == 0:\n",
    "          Bi[i] = 1\n",
    "      IDF = np.log(d / Bi)\n",
    "      \n",
    "      # Calculate TF-IDF scores\n",
    "      for j in range(d):\n",
    "        tfidf[j] = np.multiply(X[j].toarray(), IDF)\n",
    "\n",
    "      return sparse.csr_matrix(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-daZHj74o9_y"
   },
   "source": [
    "## Custom Hypergeometric Test Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3Y1lN5yYpDNr"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import hypergeom\n",
    "import itertools\n",
    "\n",
    "class HypergeomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "      min_tail_prob = 1e-250\n",
    "\n",
    "      # Initialize scoring matrix\n",
    "      d, m = X.shape\n",
    "      print('Corpus size:', d, '\\n')\n",
    "      print('Vocabulary size:', m, '\\n')\n",
    "      print('Nonzero element count:', X.count_nonzero(), '\\n')\n",
    "      \n",
    "      # Calculate base stats\n",
    "      N = X.sum()\n",
    "      n = X.sum(axis=1)\n",
    "      K = X.sum(axis=0)\n",
    "      \n",
    "      # Calculate hypergeometric test scores\n",
    "      #hgeom = sparse.csr_matrix(np.empty(shape=(d, m), dtype=np.float64))\n",
    "      hgeom_arr = np.empty(X.count_nonzero())\n",
    "      count = 0\n",
    "\n",
    "      doc_ids, term_ids = X.nonzero()\n",
    "      for doc_id, term_id in zip(doc_ids, term_ids):\n",
    "        if count % 250000 == 0:\n",
    "          print('count =', count)\n",
    "        tail_prob = hypergeom.sf(k = X[doc_id, term_id] - 1, M = N, n = K[0, term_id], N = n[doc_id, 0])\n",
    "        if np.isinf(tail_prob):\n",
    "          print('Infinity at doc_id', doc_id, ' and term_id ', term_id, '\\n')\n",
    "          exit()\n",
    "        if tail_prob < min_tail_prob:\n",
    "          hgeom_arr[count] = - np.log(min_tail_prob)\n",
    "        else:\n",
    "          hgeom_arr[count] = - np.log(tail_prob)\n",
    "        #hgeom[doc_id, term_id] = - np.log(min(tail_prob, min_tail_prob))\n",
    "        count = count + 1\n",
    "\n",
    "      unique_term_ids = set(term_ids)\n",
    "      if len(unique_term_ids) < m:\n",
    "        all_term_ids = np.array(range(m))\n",
    "        missing_term_ids = np.array(list(set(all_term_ids) - unique_term_ids))\n",
    "\n",
    "        for missing_term_id in missing_term_ids:\n",
    "          doc_ids = np.append(doc_ids, 0)\n",
    "          term_ids = np.append(term_ids, missing_term_id)\n",
    "          hgeom_arr = np.append(hgeom_arr, 0)\n",
    "\n",
    "      hgeom_coo = sparse.coo_matrix((hgeom_arr, (doc_ids, term_ids)))\n",
    "\n",
    "      hgeom = hgeom_coo.tocsr()\n",
    "\n",
    "      #return sparse.csr_matrix(hgeom)\n",
    "      return hgeom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWvkhmp81PlP"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9044,
     "status": "ok",
     "timestamp": 1684929745577,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "urQ5I1bw1UbH"
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "X_train = newsgroups_train.data\n",
    "X_test = newsgroups_test.data\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpZmkg81JSZ4"
   },
   "source": [
    "## TF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6657,
     "status": "ok",
     "timestamp": 1684837687669,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "3jpmA_G6JUqg",
    "outputId": "2e1efee0-942d-4ae0-f729-8be1c9b23f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.24      0.37       319\n",
      "           1       0.71      0.60      0.65       389\n",
      "           2       0.79      0.65      0.71       394\n",
      "           3       0.63      0.75      0.69       392\n",
      "           4       0.86      0.68      0.76       385\n",
      "           5       0.88      0.68      0.77       395\n",
      "           6       0.90      0.72      0.80       390\n",
      "           7       0.71      0.92      0.80       396\n",
      "           8       0.84      0.91      0.87       398\n",
      "           9       0.86      0.85      0.86       397\n",
      "          10       0.90      0.93      0.91       399\n",
      "          11       0.52      0.96      0.67       396\n",
      "          12       0.78      0.52      0.63       393\n",
      "          13       0.82      0.76      0.79       396\n",
      "          14       0.83      0.81      0.82       394\n",
      "          15       0.34      0.98      0.51       398\n",
      "          16       0.66      0.80      0.73       364\n",
      "          17       0.96      0.72      0.82       376\n",
      "          18       1.00      0.17      0.29       310\n",
      "          19       1.00      0.01      0.02       251\n",
      "\n",
      "    accuracy                           0.71      7532\n",
      "   macro avg       0.79      0.68      0.67      7532\n",
      "weighted avg       0.79      0.71      0.69      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB26M4b8JYj5"
   },
   "source": [
    "## TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1684676803702,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "yYcslDn8kLmj",
    "outputId": "3a36c961-0c49-4198-8088-369eae7e8ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.53      0.64       319\n",
      "           1       0.81      0.65      0.72       389\n",
      "           2       0.82      0.65      0.72       394\n",
      "           3       0.67      0.78      0.72       392\n",
      "           4       0.86      0.77      0.81       385\n",
      "           5       0.89      0.75      0.82       395\n",
      "           6       0.93      0.68      0.79       390\n",
      "           7       0.85      0.92      0.88       396\n",
      "           8       0.93      0.93      0.93       398\n",
      "           9       0.92      0.90      0.91       397\n",
      "          10       0.89      0.97      0.93       399\n",
      "          11       0.59      0.97      0.73       396\n",
      "          12       0.84      0.60      0.70       393\n",
      "          13       0.92      0.73      0.82       396\n",
      "          14       0.84      0.89      0.87       394\n",
      "          15       0.44      0.98      0.61       398\n",
      "          16       0.64      0.93      0.76       364\n",
      "          17       0.93      0.91      0.92       376\n",
      "          18       0.96      0.42      0.58       310\n",
      "          19       0.97      0.14      0.24       251\n",
      "\n",
      "    accuracy                           0.77      7532\n",
      "   macro avg       0.82      0.76      0.75      7532\n",
      "weighted avg       0.82      0.77      0.77      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3wm4rVZnT84"
   },
   "source": [
    "## Canonical TF-IDF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27644,
     "status": "ok",
     "timestamp": 1684677910745,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "xVTK0_T3mem8",
    "outputId": "94ec136c-d382-45f3-918b-f06d2e29f074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.82      0.82       319\n",
      "           1       0.62      0.77      0.69       389\n",
      "           2       0.74      0.04      0.08       394\n",
      "           3       0.53      0.78      0.63       392\n",
      "           4       0.73      0.85      0.79       385\n",
      "           5       0.78      0.76      0.77       395\n",
      "           6       0.80      0.76      0.78       390\n",
      "           7       0.87      0.92      0.89       396\n",
      "           8       0.93      0.96      0.94       398\n",
      "           9       0.95      0.94      0.94       397\n",
      "          10       0.96      0.97      0.96       399\n",
      "          11       0.87      0.92      0.89       396\n",
      "          12       0.77      0.76      0.76       393\n",
      "          13       0.90      0.82      0.86       396\n",
      "          14       0.87      0.90      0.89       394\n",
      "          15       0.86      0.94      0.90       398\n",
      "          16       0.82      0.90      0.85       364\n",
      "          17       0.96      0.91      0.94       376\n",
      "          18       0.72      0.66      0.69       310\n",
      "          19       0.70      0.64      0.67       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.80      0.79      7532\n",
      "weighted avg       0.81      0.81      0.79      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', CanonicalTfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6WQboyLJUfN"
   },
   "source": [
    "## Hypergeometric Test of Statistical Significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1160708,
     "status": "ok",
     "timestamp": 1684704665593,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "FgDU_xLSJWk3",
    "outputId": "55873225-2bd0-43f7-9ae6-9c173965c63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 11314 \n",
      "\n",
      "Vocabulary size: 130107 \n",
      "\n",
      "Nonzero element count: 1787565 \n",
      "\n",
      "count = 0\n",
      "count = 250000\n",
      "count = 500000\n",
      "count = 750000\n",
      "count = 1000000\n",
      "count = 1250000\n",
      "count = 1500000\n",
      "count = 1750000\n",
      "Corpus size: 7532 \n",
      "\n",
      "Vocabulary size: 130107 \n",
      "\n",
      "Nonzero element count: 1107956 \n",
      "\n",
      "count = 0\n",
      "count = 250000\n",
      "count = 500000\n",
      "count = 750000\n",
      "count = 1000000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       319\n",
      "           1       0.65      0.77      0.70       389\n",
      "           2       0.88      0.23      0.36       394\n",
      "           3       0.56      0.78      0.65       392\n",
      "           4       0.76      0.85      0.80       385\n",
      "           5       0.81      0.76      0.79       395\n",
      "           6       0.80      0.78      0.79       390\n",
      "           7       0.87      0.92      0.89       396\n",
      "           8       0.93      0.96      0.94       398\n",
      "           9       0.95      0.94      0.95       397\n",
      "          10       0.96      0.97      0.96       399\n",
      "          11       0.87      0.93      0.90       396\n",
      "          12       0.77      0.76      0.77       393\n",
      "          13       0.89      0.82      0.86       396\n",
      "          14       0.87      0.90      0.89       394\n",
      "          15       0.84      0.95      0.89       398\n",
      "          16       0.80      0.89      0.84       364\n",
      "          17       0.97      0.92      0.94       376\n",
      "          18       0.74      0.66      0.70       310\n",
      "          19       0.74      0.63      0.68       251\n",
      "\n",
      "    accuracy                           0.82      7532\n",
      "   macro avg       0.82      0.81      0.81      7532\n",
      "weighted avg       0.83      0.82      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', HypergeomTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
