{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cranfield Collection Preprocessing\n",
    "\n",
    "Author: Paul Sheridan\n",
    "\n",
    "Goal: Apply some basic text analysis preprocessing steps to the Cranfield collection documents/queries and output the results in JSON format.\n",
    "\n",
    "Input files (Available for download at https://github.com/oussbenk/cranfield-trec-dataset):\n",
    "    \n",
    "    * cran.all.1400.xml\n",
    "    * cran.qry.xml\n",
    "\n",
    "Output files:\n",
    "\n",
    "    * cran-docs-preprocessed.json\n",
    "    * cran-queries-preprocessed.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7217,
     "status": "ok",
     "timestamp": 1684081478145,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "yYcslDn8kLmj",
    "outputId": "73ccc51d-2f00-406b-eac2-5062c2ef89b1"
   },
   "outputs": [],
   "source": [
    "import re, string, unicodedata, xml\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom NLP functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "47ge_JRpXRzT"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    \"\"\"Stem and lemmmatize words in one go\"\"\"\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    \"\"\"Run selected above functions as a one-liner\"\"\"\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OeibF5Wnrpz"
   },
   "source": [
    "## Read Cranfield documents XML file and store its contents in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1684085369549,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "50k8b_oSkLmk",
    "outputId": "e07e8daa-ee4a-416d-a609-6db47c44eb07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1400\n"
     ]
    }
   ],
   "source": [
    "cran_all_1400_xml_file_path = 'cran.all.1400.xml' # write local path to XML file file here\n",
    "infile = open(cran_all_1400_xml_file_path, 'r')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents, 'xml')\n",
    "doc_ids = soup.find_all('docno')\n",
    "texts = soup.find_all('text')\n",
    "full_texts = []\n",
    "N = len(texts)\n",
    "print('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply NLP preprocessing steps to Cranfield docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29897,
     "status": "ok",
     "timestamp": 1684081740699,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "RMpXJZjXb8WN",
    "outputId": "278f298c-3e6f-4b4c-dde4-1b3f95bbdc40"
   },
   "outputs": [],
   "source": [
    "doc_dict = {}\n",
    "j = 0\n",
    "for text in texts:\n",
    "    doc_id = int(doc_ids[j].get_text())\n",
    "    words = nltk.word_tokenize(text.get_text())\n",
    "    words = normalize(words)\n",
    "    stems, lemmas = stem_and_lemmatize(words)\n",
    "    doc_dict[doc_id] = lemmas\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write preprocessed Cranfield docs to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cran_all_1400_json_file_path = 'cran-docs-preprocessed.json' # write local path to JSON file file here\n",
    "with open(cran_all_1400_json_file_path, 'w') as outfile:\n",
    "    json.dump(doc_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7rBU5k8jLnm"
   },
   "source": [
    "## Preprocess Cranfield queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 174,
     "status": "ok",
     "timestamp": 1684081500899,
     "user": {
      "displayName": "Paul Sheridan",
      "userId": "02245255515370102077"
     },
     "user_tz": 180
    },
    "id": "xLYKbQaVjPGD",
    "outputId": "484f5d36-6dfc-402d-d0d2-e6898cf947ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 225\n"
     ]
    }
   ],
   "source": [
    "cran_qry_xml_file_path = 'cran.qry.xml' # write local path to XML file file here\n",
    "infile = open(cran_qry_xml_file_path, 'r')\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents, 'xml')\n",
    "query_ids = soup.find_all('num')\n",
    "queries = soup.find_all('title')\n",
    "N = len(queries)\n",
    "print('N = {0}'.format(N))\n",
    "\n",
    "query_dict = {}\n",
    "j = 0\n",
    "for query in queries:\n",
    "    query_id = int(query_ids[j].get_text())\n",
    "    words = nltk.word_tokenize(query.get_text())\n",
    "    words = normalize(words)\n",
    "    stems, lemmas = stem_and_lemmatize(words)\n",
    "    query_dict[query_id] = lemmas\n",
    "    j = j + 1\n",
    "\n",
    "cran_qry_json_file_path = 'cran-queries-preprocessed.json' # write local path to JSON file file here\n",
    "with open(cran_qry_json_file_path, 'w') as outfile:\n",
    "    json.dump(query_dict, outfile)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
